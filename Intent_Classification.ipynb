{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "from nltk.tokenize import RegexpTokenizer, word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.regexp import WhitespaceTokenizer\n",
    "from nltk.stem import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Intents\n",
    "In the following list, 2 intents have been defined and a list of keywords are mentioned that identify the corresponding intent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "intents = [\n",
    "    [\"greetings\", \n",
    "     [\"hi\", \n",
    "      \"hello\"]],\n",
    "    \n",
    "    [\"farewell\", \n",
    "     [\"bye\"]]\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Responses for each Intent\n",
    "In the following code block, a list of responses are defined for each intent. Once the intent has been identified from the user's query, this list is used to return a random response from a list of corresponding responses for each intent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_responses = [\n",
    "    [\"greetings\", \n",
    "     [\"hi, how may I help you today?\", \n",
    "      \"hello, what can I do for you today?\",  \n",
    "      \"itâ€™s nice to meet you, how may we be of service?\"]],\n",
    "    \n",
    "    [\"farewell\", \n",
    "     [\"it was a pleasure to help you. do come back. type 'quit' to exit the program.\",\n",
    "      \"see you later. let me know if you have any other queries. type 'quit' to exit the program.\",\n",
    "      \"i hope the interaction was helpful. type 'quit' to exit the program.\",\n",
    "      \"thank you for your time. type 'quit' to exit the program.\"]]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing Input Text\n",
    "**Whitespace** tokenizer in the **nltk** module of python is a powerful tokenizer that can handle punctuation and contractions with greater efficiency as compared to the other tokenizers in the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_input(user_response):\n",
    "    wst = WhitespaceTokenizer()\n",
    "    return wst.tokenize(user_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stopwords\n",
    "The stopwords in English language are stored in the **corpus** module inside **nltk** library. These stopwords are removed after the text has been tokenized by the previous function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    new_tokens = list()\n",
    "    for w in tokens:\n",
    "        if w not in stop_words: new_tokens.append(w)\n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Punctuation\n",
    "After removing the stopwords, the punctuation at the end of each token - **full stop(.)**, **exclamation mark(!)**, **question mark(?)** and **comma(,)** - are removed. A simple pattern matching using Regular Expressions (**re**) module of python is sufficient to remove all such punctuation marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(user_tokens):\n",
    "    punct_re = r\"(.*)[?,.!]$\"\n",
    "    for i, word in enumerate(user_tokens):\n",
    "        if re.match(punct_re, word):\n",
    "            user_tokens[i] = word[:-1]\n",
    "    return user_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize Tokens\n",
    "Once all the unwanted punctuation is removed from the tokenized text, the **WordNetLemmatizer** is used to lemmatize (extract the root words) of each token. This step is crucial as words with similar meaning are reduced to a single word and it helps analysing text more efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_tokens(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word) for word in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard Similarity\n",
    "In this code section, a function is defined to calculate the Jaccard Similarity between 2 input sets.<br>\n",
    "Jaccard Similarity(**JS**) between 2 sets **A** and **B** is defined as - $JS = |A \\cap B| / |A \\cup B|$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jaccard_sim(set1, set2): \n",
    "    set3 = set1.intersection(set2)\n",
    "    set4 = set1.union(set2)\n",
    "    return float( len(set3) / len(set4) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching Intent\n",
    "In this section, the preprocessed input text is matched with each intent defined in the **intents** list. For each intent, the **Intent Name** and the **Jaccard Similarity** value is stored in a list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_intents(lemma_tokens):\n",
    "    intents_matched = list()\n",
    "    for intent in intents:\n",
    "        intents_matched.append([intent[0], get_jaccard_sim(set(lemma_tokens), set(intent[1]))])\n",
    "    return intents_matched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the most suited Intent\n",
    "The intent that has the maximum **Jaccard Similarity** with the user's input is extracted in the following code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_sim_intent(intents_matched):\n",
    "    max_sim = 0\n",
    "    user_intent = list()\n",
    "    for i, intent in enumerate(intents_matched):\n",
    "        if intent[1]>max_sim:\n",
    "            max_sim = intent[1]\n",
    "            user_intent = intents_matched[i]\n",
    "    return user_intent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding an appropriate response\n",
    "Once the most appropriate intent is identified, the **intent_responses** list is used to retrieve the list of corresponding responses. A random response from this list is returned to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def responses(user_intent):\n",
    "    response = str()\n",
    "    for intent_response in intent_responses:\n",
    "        if intent_response[0] == user_intent[0]:\n",
    "            response = random.choice(intent_response[1])\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the logic that genrates the bot's response\n",
    "In this function, first the user's input is preprocessed in the following manner - \n",
    "- Input is tokenized. (**tokenize_input()**)\n",
    "- Stopwords and Punctuation is removed from the tokenized input. (**remove_stopwords()** and **remove_punct()**)\n",
    "- The tokens so generated and lemmatized to generate a list of keywords for intent matching. (**lemmatize_tokens()**)\n",
    "\n",
    "After preprocessing the text, the keywords are matched to an appropriate intent in the following manner - \n",
    "- The Jaccard Similarity of the input text with all the intents are calculated. (**match_intents()**)\n",
    "- The intent with the maximum Jaccard Simmilarity is returned as the user's intent. (**max_sim_intent()**)\n",
    "\n",
    "After identifying the intent, an appropriate response is randomly selected from the list of responses stored in **intent_responses** list. (**responses()**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bot_response(user_response):\n",
    "    # Input Text Preprocessing \n",
    "    tokens = tokenize_input(user_response)\n",
    "    new_tokens = remove_stopwords(tokens)\n",
    "    new_tokens = remove_punct(new_tokens)\n",
    "    lemma_tokens = lemmatize_tokens(new_tokens)\n",
    "    \n",
    "    # Intent Matching\n",
    "    intents_matched = match_intents(lemma_tokens)\n",
    "    user_intent = max_sim_intent(intents_matched)\n",
    "    \n",
    "    # Generating an appropriate response for the intent matched\n",
    "    return responses(user_intent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Function\n",
    "The **while loop** prompts the user to enter their text as long as they type **quit**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intent Classification BOT\n",
      "\n",
      "YOU: hi\n",
      "BOT: hello, what can I do for you today?\n",
      "\n",
      "YOU: bye\n",
      "BOT: see you later. let me know if you have any other queries. type 'quit' to exit the program.\n",
      "\n",
      "YOU: quit\n",
      "BOT: Bye! take care..\n"
     ]
    }
   ],
   "source": [
    "print(\"Intent Classification BOT\")\n",
    "user_response = str()\n",
    "while(user_response!=\"quit\"):\n",
    "    print()\n",
    "    time.sleep(0.2)\n",
    "    user_response = input(\"YOU: \")\n",
    "    user_response = user_response.lower()\n",
    "    if(user_response!='quit'):\n",
    "        print(\"BOT: \"+bot_response(user_response))\n",
    "    else:\n",
    "        time.sleep(0.2)\n",
    "        print(\"BOT: Bye! take care..\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
