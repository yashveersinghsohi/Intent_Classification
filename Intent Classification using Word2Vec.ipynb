{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer, word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.regexp import WhitespaceTokenizer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import gensim\n",
    "from nltk.data import find\n",
    "from scipy import spatial\n",
    "import mysql.connector as mc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n",
    "google_news_model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_sample, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Intents\n",
    "In the following list, a few intents have been defined and a list of keywords are mentioned that identify the corresponding intent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "intents = [\n",
    "    [\"greetings\", \n",
    "    [\"Hi\", \"Hello\", \"Greetings\", \"Dear\", \"Respected\", \"Welcome\"]],\n",
    "    \n",
    "    [\"greetings_questions\", \n",
    "    [\"How are you\", \"How goes it\", \"What's up\", \"What's happening\"]],\n",
    "    \n",
    "    [\"thanking\", \n",
    "    [\"Thanks\", \"Thank you\", \"I am indebted to you\", \"I appreciate it\", \"I am grateful\", \"This is great\", \"My sincere thanks\", \"You've been very helpful\"]],\n",
    "\n",
    "    [\"apology\", \n",
    "    [\"Oops sorry\", \"Sorry about that\", \"I'm sorry\", \"sorry\", \"Sorry\", \"My bad\", \"Apologies\", \"I apologize\"]],\n",
    "\n",
    "    [\"farewell\", \n",
    "    [\"Bye\", \"Farewell\", \"See you later\", \"Talk to you later\", \"Goodbye\", \"Take care\", \"Nice to meet you\", \"Nice to talk to you\"]]\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Responses for each Intent\n",
    "In the following code block, a list of responses are defined for each intent. Once the intent has been identified from the user's query, this list is used to return a random response from a list of corresponding responses for each intent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_responses = {\n",
    "    \"greetings\": [\"hi, how may I help you today?\", \n",
    "                  \"hello, what can I do for you today?\",  \n",
    "                  \"itâ€™s nice to meet you, how may we be of service?\"],\n",
    "    \n",
    "    \"greetings_questions\": [\"I'm great. How  can I help?\"],\n",
    "    \n",
    "    \"thanking\": [\"don't mention it.\"],\n",
    "    \n",
    "    \"apology\": [\"hey, don't worry about it.\", \n",
    "                \"it's ok. no need to apologize\"],\n",
    "    \n",
    "    \"farewell\": [\"it was a pleasure to help you. do come back. type 'quit' to exit the program.\",\n",
    "                 \"see you later. let me know if you have any other queries. type 'quit' to exit the program.\",\n",
    "                 \"i hope the interaction was helpful. type 'quit' to exit the program.\",\n",
    "                 \"thank you for your time. type 'quit' to exit the program.\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_responses = [\n",
    "    \"i am sorry. i am not sure about what you wanted to say.\",\n",
    "    \"i am sorry. it is becomming a bit hard to follow along.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing Input Text\n",
    "**Whitespace** tokenizer in the **nltk** module of python is a powerful tokenizer that can handle punctuation and contractions with greater efficiency as compared to the other tokenizers in the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_input(user_response):\n",
    "    wst = WhitespaceTokenizer()\n",
    "    return wst.tokenize(user_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Punctuation\n",
    "The punctuation at the end of each token - **full stop(.)**, **exclamation mark(!)**, **question mark(?)** and **comma(,)** - are removed. A simple pattern matching using Regular Expressions (**re**) module of python is sufficient to remove all such punctuation marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(user_tokens):\n",
    "    punct_re = r\"(.*)[?,.!]$\"\n",
    "    for i, word in enumerate(user_tokens):\n",
    "        if re.match(punct_re, word):\n",
    "            user_tokens[i] = word[:-1]\n",
    "    return user_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stopwords\n",
    "After removing the punctuations from each token, some of the common tokens with limited significance in determining the intent of the statement are removed. Such common tokens are known as stopwords. The stopwords in English language are stored in the **corpus** module inside **nltk** library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    new_tokens = list()\n",
    "    for w in tokens:\n",
    "        if w not in stop_words: new_tokens.append(w)\n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize Tokens\n",
    "Once all the unwanted punctuation is removed from the tokenized text, the **WordNetLemmatizer** is used to lemmatize (extract the root words) of each token. This step is crucial as words with similar meaning are reduced to a single word and it helps analysing text more efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_tokens(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word) for word in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Word Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each word is checked against all the words in words in the pruned set of word vectors from the word2vec model. If the word, or any of its variants(upper case, lower case, title etc.) are present then the vector is returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_token_vec(token):\n",
    "    token_vec = list()\n",
    "    if token in google_news_model:\n",
    "        token_vec = google_news_model[token]\n",
    "\n",
    "    elif token.istitle():\n",
    "        if token.lower() in google_news_model:\n",
    "            token_vec = google_news_model[token.lower()]\n",
    "        else:\n",
    "            print(\"not in vocab\")\n",
    "\n",
    "    elif token.islower():\n",
    "        if token.capitalize() in google_news_model:\n",
    "            token_vec = google_news_model[token.capitalize()]\n",
    "        else:\n",
    "            print(\"Not In Vocab\")\n",
    "\n",
    "    return token_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Phrase Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word vectors build are added to build a phrase vector. In this way, the affects of a set of words are captured using arithmetic operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_phrase_vec(phrase):\n",
    "    token_vec_list = list()\n",
    "    phrase_tokens = phrase.split()\n",
    "    \n",
    "    for token in phrase_tokens:\n",
    "        token_vec = build_token_vec(token)\n",
    "        token_vec_list.append(token_vec)\n",
    "    \n",
    "    phrase_vec = sum(token_vec_list)\n",
    "    return phrase_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Intents Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step the words or phrases representing each intent are vectorized and stored. Later an input word/phrase is vectorized and compared with each representative of each intent using Euclidean Distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_intents_vec(intents):\n",
    "    \n",
    "    intent_vec_dict = dict()\n",
    "    \n",
    "    for intent in intents:\n",
    "        intent_name, intent_key_words = intent\n",
    "        intent_vec_dict.update({intent_name: []})\n",
    "        \n",
    "        for phrase in intent_key_words:\n",
    "            phrase_vec = build_phrase_vec(phrase)\n",
    "            intent_vec_dict[intent_name].append([phrase, phrase_vec])\n",
    "    return intent_vec_dict "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_euclid_dist(key_vec, phrase_vec):\n",
    "    dist_list = list()\n",
    "    for key in key_vec:\n",
    "        dist_list.append(np.linalg.norm(key[1]-phrase_vec))\n",
    "    \n",
    "    return min(dist_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching Intent\n",
    "In this section, the preprocessed input text is matched with each intent defined in the **intents** list. For each intent, the **Intent Name** and the **Jaccard Similarity** value is stored in a list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_intents(lemma_tokens, intents):\n",
    "    intent_vec_dict = build_intents_vec(intents)\n",
    "\n",
    "    phrase = \" \"\n",
    "    phrase = phrase.join(lemma_tokens)\n",
    "    phrase_vec = build_phrase_vec(phrase)\n",
    "    \n",
    "    intents_matched = list()\n",
    "    for intent, key_vec in intent_vec_dict.items():\n",
    "        intents_matched.append([intent, get_euclid_dist(key_vec, phrase_vec)])\n",
    "    \n",
    "    return intents_matched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the most suited Intent\n",
    "The intent that has the minimum **Eucledian Distance** with the user's input is extracted in the following code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_dist_intent(intents_matched):\n",
    "    min_dist = float('inf')\n",
    "    user_intent = list()\n",
    "    for i, intent in enumerate(intents_matched):\n",
    "        if intent[1]<min_dist:\n",
    "            min_dist = intent[1]\n",
    "            user_intent = intents_matched[i]\n",
    "    return user_intent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding an appropriate response\n",
    "Once the most appropriate intent is identified, the **intent_responses** list is used to retrieve the list of corresponding responses. A random response from this list is returned to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def responses(user_intent):\n",
    "    if user_intent==list():\n",
    "        print(random.choice(default_responses))\n",
    "        return\n",
    "    \n",
    "    print(random.choice(intent_responses[user_intent[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the logic that genrates the bot's response\n",
    "In this function, first the user's input is preprocessed in the following manner - \n",
    "- Input is tokenized. (**tokenize_input()**)\n",
    "- Stopwords and Punctuation is removed from the tokenized input. (**remove_stopwords()** and **remove_punct()**)\n",
    "- The tokens so generated and lemmatized to generate a list of keywords for intent matching. (**lemmatize_tokens()**)\n",
    "\n",
    "After preprocessing the text, the keywords are matched to an appropriate intent in the following manner - \n",
    "- The Cosine Similarity of the input text with all the intents is calculated. (**match_intents()**)\n",
    "- The intent with the maximum Cosine Simmilarity is returned as the user's intent. (**max_sim_intent()**)\n",
    "\n",
    "After identifying the intent, an appropriate response is randomly selected from the list of responses stored in **intent_responses** list. (**responses()**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bot_response(user_response, intents):\n",
    "    # Input Text Preprocessing \n",
    "    tokens = tokenize_input(user_response)\n",
    "    new_tokens = remove_punct(tokens)\n",
    "    # new_tokens = remove_stopwords(new_tokens)\n",
    "    lemma_tokens = lemmatize_tokens(new_tokens)\n",
    "    \n",
    "    # Intent Matching\n",
    "    intents_matched = match_intents(lemma_tokens, intents)\n",
    "    # print(intents_matched)\n",
    "    user_intent = min_dist_intent(intents_matched)\n",
    "    # print(user_intent)\n",
    "    \n",
    "    # Generating an appropriate response for the intent matched\n",
    "    responses(user_intent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Function\n",
    "The **while loop** prompts the user to enter their text until they type **quit**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type 'quit' to exit the conversation\n",
      "\n",
      "YOU: hi\n",
      "BOT: itâ€™s nice to meet you, how may we be of service?\n",
      "\n",
      "\n",
      "YOU: thank you for your time\n",
      "BOT: don't mention it.\n",
      "\n",
      "\n",
      "YOU: bye\n",
      "BOT: see you later. let me know if you have any other queries. type 'quit' to exit the program.\n",
      "\n",
      "\n",
      "YOU: quit\n",
      "BOT: Bye! take care..\n"
     ]
    }
   ],
   "source": [
    "print(\"Type 'quit' to exit the conversation\")\n",
    "user_response = str()\n",
    "while(user_response!=\"quit\"):\n",
    "    print()\n",
    "    time.sleep(0.2)\n",
    "    user_response = input(\"YOU: \")\n",
    "    user_response = user_response.lower()\n",
    "    if(user_response!='quit'):\n",
    "        # print(\"BOT: \"+bot_response(user_response, intents))\n",
    "        print(\"BOT: \", end='')\n",
    "        bot_response(user_response, intents)\n",
    "        print()\n",
    "    else:\n",
    "        time.sleep(0.2)\n",
    "        print(\"BOT: Bye! take care..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
